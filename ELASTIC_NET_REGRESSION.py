import numpy as np

# ELASTIC_NET_REGRESSION: IMPLEMENTATION OF ELASTIC NET REGRESSION. ELASTIC-NET IS A LINEAR REGRESSION MODEL TRAINED WITH BOTH L1 AND L2-NORM REGULARIZATION OF THE COEFFICIENTS. THIS COMBINATION ALLOWS FOR LEARNING A SPARSE MODEL WHERE FEW OF THE WEIGHTS ARE NON-ZERO LIKE LASSO, WHILE STILL MAINTAINING THE REGULARIZATION PROPERTIES OF RIDGE. WE CONTROL THE CONVEX COMBINATION OF L1 AND L2 USING THE L1_RATIO PARAMETER. ELASTIC-NET IS USEFUL WHEN THERE ARE MULTIPLE FEATURES THAT ARE CORRELATED WITH ONE ANOTHER. LASSO IS LIKELY TO PICK ONE OF THESE AT RANDOM, WHILE ELASTIC-NET IS LIKELY TO PICK BOTH. A PRACTICAL ADVANTAGE OF TRADING-OFF BETWEEN LASSO AND RIDGE IS THAT IT ALLOWS ELASTIC-NET TO INHERIT SOME OF RIDGEâ€™S STABILITY UNDER ROTATION. MEANING, IT'S A LINEAR MODEL TRAINED WITH L1 AND L2 PRIORS AS REGULARIZERS.
class ELASTIC_NET_REGRESSION:
    # INITIALIZES THE ELASTIC NET REGRESSION MODEL.
    def __init__(self, FIT_INTERCEPT=True, ALPHA=1.0, L1_RATIO=0.5):
        # FIT_INTERCEPT: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO WHETHER TO CALCULATE THE INTERCEPT FOR THIS MODEL. IF SET TO FALSE, NO INTERCEPT WILL BE USED IN CALCULATIONS (E.G. DATA IS EXPECTED TO BE ALREADY CENTERED).
        self.FIT_INTERCEPT = FIT_INTERCEPT
        # ALPHA: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE REGULARIZATION STRENGTH. IT MUST BE A POSITIVE FLOAT. REGULARIZATION IMPROVES THE CONDITIONING OF THE PROBLEM AND REDUCES THE VARIANCE OF THE ESTIMATES. LARGER VALUES SPECIFY STRONGER REGULARIZATION.
        self.ALPHA = ALPHA
        # L1_RATIO: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE MIXING PARAMETER FOR THE REGULARIZATION. WHEN L1_RATIO = 0 THE PENALTY IS AN L2 PENALTY. WHEN L1_RATIO = 1 IT IS AN L1 PENALTY. WHEN 0 < L1_RATIO < 1, THE PENALTY IS A COMBINATION OF L1 AND L2.
        self.L1_RATIO = L1_RATIO
        # WEIGHTS: IT'S THE PARAMETER THAT CORRESPONDS TO THE WEIGHTS OF THE LINEAR REGRESSION MODEL.
        self.WEIGHTS = None
        # BIAS: IT'S THE PARAMETER THAT CORRESPONDS TO THE BIAS OF THE LINEAR REGRESSION MODEL.
        self.BIAS = None

    # FIT(): IT'S THE FUNCTION THAT TRAINS THE LINEAR REGRESSION MODEL.
    def FIT(self, X, Y):
        _, D = X.shape  # N: NUMBER OF SAMPLES. D: NUMBER OF FEATURES.
        # CALCULATES THE WEIGHTS OF THE LINEAR REGRESSION MODEL.
        self.WEIGHTS = np.linalg.inv(X.T @ X + self.ALPHA * (
            (1 - self.L1_RATIO) * np.identity(D) + self.L1_RATIO * np.ones((D, D)))) @ X.T @ Y
        # CALCULATES THE BIAS OF THE LINEAR REGRESSION MODEL.
        if self.FIT_INTERCEPT == True:  # IF FIT_INTERCEPT IS TRUE.
            # CALCULATES THE BIAS OF THE LINEAR REGRESSION MODEL.
            self.BIAS = np.mean(Y, axis=0) - np.mean(X @ self.WEIGHTS, axis=0)
        else:  # IF FIT_INTERCEPT IS FALSE.
            # CALCULATES THE BIAS OF THE LINEAR REGRESSION MODEL.
            self.BIAS = np.zeros((1, Y.shape[1]))

    # TRANSFORM(): IT'S THE FUNCTION THAT USES THE LINEAR REGRESSION MODEL TO PREDICT NEW VALUES.
    def TRANSFORM(self, X):
        return X @ self.WEIGHTS + self.BIAS  # RETURNS THE PREDICTED VALUES.