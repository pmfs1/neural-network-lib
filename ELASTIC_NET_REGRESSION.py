import numpy as np

# ELASTIC_NET_REGRESSION: IMPLEMENTATION OF ELASTIC NET REGRESSION. ELASTIC-NET IS A LINEAR REGRESSION MODEL TRAINED WITH BOTH L1 AND L2-NORM REGULARIZATION OF THE COEFFICIENTS. THIS COMBINATION ALLOWS FOR LEARNING A SPARSE MODEL WHERE FEW OF THE WEIGHTS ARE NON-ZERO LIKE LASSO, WHILE STILL MAINTAINING THE REGULARIZATION PROPERTIES OF RIDGE. WE CONTROL THE CONVEX COMBINATION OF L1 AND L2 USING THE L1_RATIO PARAMETER. ELASTIC-NET IS USEFUL WHEN THERE ARE MULTIPLE FEATURES THAT ARE CORRELATED WITH ONE ANOTHER. LASSO IS LIKELY TO PICK ONE OF THESE AT RANDOM, WHILE ELASTIC-NET IS LIKELY TO PICK BOTH. A PRACTICAL ADVANTAGE OF TRADING-OFF BETWEEN LASSO AND RIDGE IS THAT IT ALLOWS ELASTIC-NET TO INHERIT SOME OF RIDGEâ€™S STABILITY UNDER ROTATION. MEANING, IT'S A LINEAR MODEL TRAINED WITH L1 AND L2 PRIORS AS REGULARIZERS.
class ELASTIC_NET_REGRESSION:
    # INITIALIZES THE ELASTIC NET REGRESSION MODEL.
    def __init__(self, LEARNING_RATE=0.01, ITERATIONS=1000, L1_PENALITY=1, L2_PENALITY=0.01):
        # LEARNING_RATE: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE LEARNING RATE. IT CONTROLS HOW MUCH TO CHANGE THE MODEL IN RESPONSE TO THE ESTIMATED ERROR EACH TIME THE MODEL WEIGHTS ARE UPDATED.
        self.LEARNING_RATE = LEARNING_RATE
        # ITERATIONS: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE NUMBER OF ITERATIONS. IT CONTROLS THE NUMBER OF TIMES TO REPEAT THE PROCESS OF LEARNING THE WEIGHTS.
        self.ITERATIONS = ITERATIONS
        # L1_PENALITY: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE L1 PENALITY. IT CONTROLS THE STRENGTH OF THE L1 PENALTY TERM.
        self.L1_PENALITY = L1_PENALITY
        # L2_PENALITY: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE L2 PENALITY. IT CONTROLS THE STRENGTH OF THE L2 PENALTY TERM.
        self.L2_PENALITY = L2_PENALITY
        # WEIGHTS: IT'S THE PARAMETER THAT CORRESPONDS TO THE WEIGHTS OF THE LINEAR REGRESSION MODEL.
        self.WEIGHTS = None
        # BIAS: IT'S THE PARAMETER THAT CORRESPONDS TO THE BIAS OF THE LINEAR REGRESSION MODEL.
        self.BIAS = 0

    # FIT(): IMPLEMENTS THE FITTING FUNCTIONALITY FOR THE ELASTIC NET REGRESSION MODEL.
    def FIT(self, X, Y):
        # CALCULATE THE WEIGHTS OF THE LINEAR REGRESSION MODEL.
        self.WEIGHTS = np.zeros(X.shape[1])
        # ITERATE THROUGH THE NUMBER OF ITERATIONS.
        for _ in range(self.ITERATIONS):
            # CALCULATE THE PREDICTIONS OF THE LINEAR REGRESSION MODEL.
            Y_PRED = self.TRANSFORM(X)
            # INITIALIZE THE DERIVATIVE OF THE WEIGHTS.
            DERIVATIVE_WEIGHTS = np.zeros(X.shape[1])
            # ITERATE THROUGH THE NUMBER OF FEATURES.
            for J in range(X.shape[1]):
                # CALCULATE THE DERIVATIVE OF THE WEIGHTS.
                if self.WEIGHTS[J] > 0:  # IF THE WEIGHT IS POSITIVE.
                    # CALCULATE THE DERIVATIVE OF THE WEIGHTS: L1 PENALITY + L2 PENALITY.
                    DERIVATIVE_WEIGHTS[J] = (-2 * (X[:, J]).dot(
                        Y - Y_PRED) + self.L1_PENALITY + 2 * self.L2_PENALITY * self.WEIGHTS[J]) / X.shape[0]
                else:  # IF THE WEIGHT IS NEGATIVE.
                    # CALCULATE THE DERIVATIVE OF THE WEIGHTS: L1 PENALITY - L2 PENALITY.
                    DERIVATIVE_WEIGHTS[J] = (-2 * (X[:, J]).dot(
                        Y - Y_PRED) - self.L1_PENALITY + 2 * self.L2_PENALITY * self.WEIGHTS[J]) / X.shape[0]
            # CALCULATE THE DERIVATIVE OF THE BIAS.
            DERIVATIVE_BIAS = -2 * np.sum(Y - Y_PRED) / X.shape[0]
            # UPDATE THE WEIGHTS.
            self.WEIGHTS = self.WEIGHTS - self.LEARNING_RATE * DERIVATIVE_WEIGHTS
            # UPDATE THE BIAS.
            self.BIAS = self.BIAS - self.LEARNING_RATE * DERIVATIVE_BIAS

    # TRANSFORM(): PREDICTS THE LABELS OF THE TRAINING DATA.
    def TRANSFORM(self, X):
        # RETURNS THE PREDICTIONS OF THE LINEAR REGRESSION MODEL.
        return X.dot(self.WEIGHTS) + self.BIAS