import numpy as np
from collections import Counter

# DECISION_TREE.NODE: CLASS THAT IMPLEMENTS A NODE OF A DECISION TREE
class NODE:
    # INITIALIZES THE NODE OF A DECISION TREE
    def __init__(self, FEATURE=None, THRESHOLD=None, LEFT=None, RIGHT=None, *, VALUE=None):
        # FEATURE: IT'S THE FEATURE THAT THE NODE USES TO MAKE A DECISION.
        self.FEATURE = FEATURE
        # THRESHOLD: IT'S THE THRESHOLD THAT THE NODE USES TO MAKE A DECISION.
        self.THRESHOLD = THRESHOLD
        self.LEFT = LEFT  # LEFT: IT'S THE LEFT CHILD OF THE NODE.
        self.RIGHT = RIGHT  # RIGHT: IT'S THE RIGHT CHILD OF THE NODE.
        self.VALUE = VALUE  # VALUE: IT'S THE VALUE OF THE NODE.

    # IS_LEAF_NODE(): METHOD THAT CHECKS IF THE NODE IS A LEAF NODE
    def IS_LEAF_NODE(self):
        return self.VALUE is not None  # RETURNS IF THE NODE IS A LEAF NODE; TRUE OR FALSE

# DECISION_TREE: CLASS THAT IMPLEMENTS DECISION TREE MODEL
class DECISION_TREE:
    # INITIALIZES THE DECISION TREE MODEL
    def __init__(self, MIN_SAMPLES_SPLIT=2, MAX_DEPTH=100, N_FEATURES=None):
        # MIN_SAMPLES_SPLIT: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE MINIMUM NUMBER OF SAMPLES REQUIRED TO SPLIT AN INTERNAL NODE.
        self.MIN_SAMPLES_SPLIT = MIN_SAMPLES_SPLIT
        # MAX_DEPTH: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE MAXIMUM DEPTH OF THE TREE.
        self.MAX_DEPTH = MAX_DEPTH
        # N_FEATURES: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE NUMBER OF FEATURES.
        self.N_FEATURES = N_FEATURES
        self.ROOT = None  # ROOT: IT'S THE ROOT OF THE TREE.

    # FIT(): METHOD THAT TRAINS THE DECISION TREE MODEL
    def FIT(self, X, Y):
        # N_FEATURES: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE NUMBER OF FEATURES. IT'S EQUAL TO THE NUMBER OF FEATURES OF THE DATASET IF N_FEATURES IS NONE, OTHERWISE IT'S EQUAL TO THE MINIMUM BETWEEN N_FEATURES AND THE NUMBER OF FEATURES OF THE DATASET.
        self.N_FEATURES = X.shape[1] if not self.N_FEATURES else min(
            self.N_FEATURES, X.shape[1])
        # ROOT: IT'S THE ROOT OF THE TREE. IT'S THE OUTPUT OF THE __GROW_TREE__() FUNCTION.
        self.ROOT = self.__GROW_TREE__(X, Y)

    # TRANSFORM(): METHOD THAT TRANSFORMS THE DATASET
    def TRANSFORM(self, X):
        # RETURNS THE TRANSFORMED DATASET: [TRAVERSE_TREE(X[I], ROOT) FOR X[I] IN X]. IT'S THE OUTPUT OF THE __TRAVERSE_TREE__() FUNCTION.
        return np.array([self.__TRAVERSE_TREE__(X_I, self.ROOT) for X_I in X])

    # __GROW_TREE__() [PRIVATE METHOD]: METHOD THAT GROWS THE TREE
    def __GROW_TREE__(self, X, Y, DEPTH=0):
        # NUMBER OF TRAINING EXAMPLES: IT'S THE NUMBER OF ROWS OF THE DATASET; NUMBER OF FEATURES: IT'S THE NUMBER OF COLUMNS OF THE DATASET.
        NUMBER_OF_SAMPLES, NUMBER_OF_FEATURES = X.shape
        # NUMBER OF LABELS: IT'S THE NUMBER OF UNIQUE LABELS IN THE DATASET.
        NUMBER_OF_LABELS = len(np.unique(Y))
        # STOPPING CRITERIA
        # IF DEPTH >= MAX_DEPTH OR NUMBER_OF_LABELS == 1 OR NUMBER_OF_SAMPLES < MIN_SAMPLES_SPLIT, THEN
        if (DEPTH >= self.MAX_DEPTH) or (NUMBER_OF_LABELS == 1) or (NUMBER_OF_SAMPLES < self.MIN_SAMPLES_SPLIT):
            # LEAF_VALUE: IT'S THE MOST COMMON LABEL IN THE DATASET. IT'S THE OUTPUT OF THE __MOST_COMMON_LABEL__() FUNCTION.
            LEAF_VALUE = self.__MOST_COMMON_LABEL__(Y)
            return NODE(LEAF_VALUE)  # RETURNS A NODE WITH LEAF_VALUE AS VALUE.
        # FEATURE_INDEXES: IT'S A LIST OF N_FEATURES RANDOMLY SELECTED FEATURES.
        FEATURE_INDEXES = np.random.choice(
            NUMBER_OF_FEATURES, self.N_FEATURES, replace=False)
        # GREEDILY SELECT THE BEST SPLIT ACCORDING TO INFORMATION GAIN
        # BEST_FEATURE_INDEX: IT'S THE INDEX OF THE FEATURE THAT MAXIMIZES THE INFORMATION GAIN; BEST_THRESHOLD: IT'S THE THRESHOLD THAT MAXIMIZES THE INFORMATION GAIN. THEY ARE THE OUTPUTS OF THE __BEST_CRITERIA__() FUNCTION.
        BEST_FEATURE_INDEX, BEST_THRESHOLD = self.__BEST_CRITERIA__(
            X, Y, FEATURE_INDEXES)
        # GROW THE CHILDREN THAT RESULT FROM THE SPLIT
        # LEFT_INDEXES: IT'S THE LIST OF INDEXES OF THE LEFT CHILDREN; RIGHT_INDEXES: IT'S THE LIST OF INDEXES OF THE RIGHT CHILDREN. THEY ARE THE OUTPUTS OF THE __SPLIT__() FUNCTION.
        LEFT_INDEXES, RIGHT_INDEXES = self.__SPLIT__(
            X[:, BEST_FEATURE_INDEX], BEST_THRESHOLD)
        # LEFT: IT'S THE LEFT CHILD. IT'S THE OUTPUT OF THE __GROW_TREE__() FUNCTION.
        LEFT = self.__GROW_TREE__(
            X[LEFT_INDEXES, :], Y[LEFT_INDEXES], DEPTH + 1)
        # RIGHT: IT'S THE RIGHT CHILD. IT'S THE OUTPUT OF THE __GROW_TREE__() FUNCTION.
        RIGHT = self.__GROW_TREE__(
            X[RIGHT_INDEXES, :], Y[RIGHT_INDEXES], DEPTH + 1)
        # RETURNS A NODE WITH BEST_FEATURE_INDEX AS FEATURE_INDEX, BEST_THRESHOLD AS THRESHOLD, LEFT AS LEFT CHILD AND RIGHT AS RIGHT CHILD.
        return NODE(BEST_FEATURE_INDEX, BEST_THRESHOLD, LEFT, RIGHT)

    # __TRAVERSE_TREE__() [PRIVATE METHOD]: METHOD THAT TRAVERSES THE TREE
    def __TRAVERSE_TREE__(self, X, NODE):
        if NODE.LEAF_VALUE is not None:  # IF NODE.LEAF_VALUE IS NOT NONE, THEN
            return NODE.LEAF_VALUE  # RETURNS NODE.LEAF_VALUE
        # FEATURE_VALUE: IT'S THE VALUE OF THE FEATURE WITH INDEX NODE.FEATURE_INDEX IN THE INPUT X.
        FEATURE_VALUE = X[NODE.FEATURE_INDEX]
        # CHILD: IT'S THE LEFT CHILD IF FEATURE_VALUE <= NODE.THRESHOLD, OTHERWISE IT'S THE RIGHT CHILD.
        CHILD = NODE.LEFT if FEATURE_VALUE <= NODE.THRESHOLD else NODE.RIGHT
        # RETURNS THE OUTPUT OF THE __TRAVERSE_TREE__() FUNCTION.
        return self.__TRAVERSE_TREE__(X, CHILD)

    # __TRAVERSE_TREE__() [PRIVATE METHOD]: METHOD THAT TRAVERSES THE TREE
    # def __TRAVERSE_TREE__(self, X, NODE):
    #     if NODE.LEAF_VALUE is not None: # IF NODE.LEAF_VALUE IS NOT NONE, THEN
    #         return NODE.LEAF_VALUE # RETURNS NODE.LEAF_VALUE
    #     if X[NODE.FEATURE] <= NODE.THRESHOLD: # IF X[NODE.feature] <= NODE.THRESHOLD, THEN
    #         return self.__TRAVERSE_TREE__(X, NODE.LEFT) # RETURNS THE OUTPUT OF THE _TRAVERSE_TREE() FUNCTION.
    #     return self.__TRAVERSE_TREE__(X, NODE.RIGHT) # RETURNS THE OUTPUT OF THE _TRAVERSE_TREE() FUNCTION.

    # __MOST_COMMON_LABEL__() [PRIVATE METHOD]: METHOD THAT RETURNS THE MOST COMMON LABEL IN THE DATASET~
    def __MOST_COMMON_LABEL__(self, Y):
        # RETURNS THE MOST COMMON LABEL IN THE DATASET.
        return Counter(Y).most_common(1)[0][0]

    # __BEST_CRITERIA__() [PRIVATE METHOD]: METHOD THAT RETURNS THE BEST CRITERIA
    def __BEST_CRITERIA__(self, X, Y, FEATURE_INDEXES):
        # BEST_GAIN: IT'S THE BEST GAIN. IT'S INITIALIZED TO -1.
        BEST_GAIN = -1
        # BEST_FEATURE_INDEX: IT'S THE INDEX OF THE FEATURE THAT MAXIMIZES THE INFORMATION GAIN; BEST_THRESHOLD: IT'S THE THRESHOLD THAT MAXIMIZES THE INFORMATION GAIN. THEY ARE INITIALIZED TO NONE.
        BEST_FEATURE_INDEX, BEST_THRESHOLD = None, None
        for FEATURE_INDEX in FEATURE_INDEXES:  # FOR EACH FEATURE_INDEX IN FEATURE_INDEXES, THEN
            # X_COLUMN: IT'S THE COLUMN OF X WITH INDEX FEATURE_INDEX.
            X_COLUMN = X[:, FEATURE_INDEX]
            # THRESHOLDS: IT'S THE LIST OF UNIQUE VALUES IN X_COLUMN.
            THRESHOLDS = np.unique(X_COLUMN)
            for THRESHOLD in THRESHOLDS:  # FOR EACH THRESHOLD IN THRESHOLDS, THEN
                # GAIN: IT'S THE INFORMATION GAIN. IT'S THE OUTPUT OF THE __INFORMATION_GAIN__() FUNCTION.
                GAIN = self.__INFORMATION_GAIN__(Y, X_COLUMN, THRESHOLD)
                if GAIN > BEST_GAIN:  # IF GAIN > BEST_GAIN, THEN
                    BEST_GAIN = GAIN  # BEST_GAIN = GAIN
                    BEST_FEATURE_INDEX = FEATURE_INDEX  # BEST_FEATURE_INDEX = FEATURE_INDEX
                    BEST_THRESHOLD = THRESHOLD  # BEST_THRESHOLD = THRESHOLD
        # RETURNS BEST_FEATURE_INDEX AND BEST_THRESHOLD.
        return BEST_FEATURE_INDEX, BEST_THRESHOLD

    # __INFORMATION_GAIN__() [PRIVATE METHOD]: METHOD THAT RETURNS THE INFORMATION GAIN
    def __INFORMATION_GAIN__(self, Y, X_COLUMN, SPLIT_THRESHOLD):
        # PARENT_ENTROPY: IT'S THE ENTROPY OF THE PARENT. IT'S THE OUTPUT OF THE ENTROPY() FUNCTION.
        PARENT_ENTROPY = self.ENTROPY(Y)
        # LEFT_INDEXES: IT'S THE LIST OF INDEXES OF THE LEFT CHILDREN; RIGHT_INDEXES: IT'S THE LIST OF INDEXES OF THE RIGHT CHILDREN. THEY ARE THE OUTPUTS OF THE __SPLIT__() FUNCTION.
        LEFT_INDEXES, RIGHT_INDEXES = self.__SPLIT__(X_COLUMN, SPLIT_THRESHOLD)
        # IF THE LENGTH OF LEFT_INDEXES IS 0 OR THE LENGTH OF RIGHT_INDEXES IS 0, THEN
        if len(LEFT_INDEXES) == 0 or len(RIGHT_INDEXES) == 0:
            return 0  # RETURNS 0.
        N = len(Y)  # N: IT'S THE LENGTH OF Y.
        # N_L: IT'S THE LENGTH OF LEFT_INDEXES; N_R: IT'S THE LENGTH OF RIGHT_INDEXES.
        N_L, N_R = len(LEFT_INDEXES), len(RIGHT_INDEXES)
        # CHILDREN_ENTROPY: IT'S THE ENTROPY OF THE CHILDREN. IT'S THE SUM OF THE PRODUCT OF THE PROBABILITY OF THE LEFT CHILDREN AND THE ENTROPY OF THE LEFT CHILDREN AND THE PRODUCT OF THE PROBABILITY OF THE RIGHT CHILDREN AND THE ENTROPY OF THE RIGHT CHILDREN.
        CHILDREN_ENTROPY = (
            N_L / N) * self.ENTROPY(Y[LEFT_INDEXES]) + (N_R / N) * self.ENTROPY(Y[RIGHT_INDEXES])
        # RETURNS THE INFORMATION GAIN.
        return PARENT_ENTROPY - CHILDREN_ENTROPY

    # __SPLIT__() [PRIVATE METHOD]: METHOD THAT SPLITS THE DATASET
    def __SPLIT__(self, X_COLUMN, SPLIT_THRESHOLD):
        # LEFT_INDEXES: IT'S THE LIST OF INDEXES OF THE LEFT CHILDREN. IT'S THE INDEXES OF THE ELEMENTS IN X_COLUMN THAT ARE LESS THAN OR EQUAL TO SPLIT_THRESHOLD.
        LEFT_INDEXES = np.argwhere(X_COLUMN <= SPLIT_THRESHOLD).flatten()
        # RIGHT_INDEXES: IT'S THE LIST OF INDEXES OF THE RIGHT CHILDREN. IT'S THE INDEXES OF THE ELEMENTS IN X_COLUMN THAT ARE GREATER THAN SPLIT_THRESHOLD.
        RIGHT_INDEXES = np.argwhere(X_COLUMN > SPLIT_THRESHOLD).flatten()
        # RETURNS LEFT_INDEXES AND RIGHT_INDEXES.
        return LEFT_INDEXES, RIGHT_INDEXES

    # ENTROPY() [STATIC METHOD]: METHOD THAT RETURNS THE ENTROPY

    @staticmethod
    def ENTROPY(Y):
        # RETURNS THE ENTROPY. IT'S THE SUM OF THE NEGATIVE OF THE PRODUCT OF P AND LOG2(P) FOR EACH P IN THE LIST OF THE PROBABILITIES OF THE LABELS IN Y.
        return -np.sum([P * np.log2(P) for P in (np.bincount(Y) / len(Y)) if P > 0])