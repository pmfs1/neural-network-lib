import numpy as np

# BAYESIAN_RIDGE_REGRESSION: IMPLEMENTS AN ALGORITHM FOR REGULARIZED BAYESIAN LINEAR REGRESSION FIT. IT USES A HYPERPARAMETER TO SPECIFY REGULARIZATION STRENGTH AND PERFORMS FULL HYPERPARAMETER REGULARIZATION WITH AN APPROXIMATELY NONINFORMATIVE PRIOR.
class BAYESIAN_RIDGE_REGRESSION:
    # INITIALIZES THE BAYESIAN RIDGE REGRESSION MODEL
    def __init__(self, ALPHA_1=1e-6, ALPHA_2=1e-6, LAMBDA_1=1e-6, LAMBDA_2=1e-6, N_ITER=300):
        # ALPHA_1: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE SHAPE PARAMETER FOR THE GAMMA DISTRIBUTION USED FOR THE PRECISION OF THE WEIGHTS.
        self.ALPHA_1 = ALPHA_1
        # ALPHA_2: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE RATE PARAMETER FOR THE GAMMA DISTRIBUTION USED FOR THE PRECISION OF THE WEIGHTS.
        self.ALPHA_2 = ALPHA_2
        # LAMBDA_1: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE SHAPE PARAMETER FOR THE GAMMA DISTRIBUTION USED FOR THE PRECISION OF THE NOISE.
        self.LAMBDA_1 = LAMBDA_1
        # LAMBDA_2: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE RATE PARAMETER FOR THE GAMMA DISTRIBUTION USED FOR THE PRECISION OF THE NOISE.
        self.LAMBDA_2 = LAMBDA_2
        # N_ITER: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE NUMBER OF ITERATIONS FOR THE OPTIMIZATION.
        self.N_ITER = N_ITER
        # WEIGHTS: IT'S THE PARAMETER THAT CORRESPONDS TO THE WEIGHTS OF THE LINEAR REGRESSION MODEL.
        self.WEIGHTS = None
        # BIAS: IT'S THE PARAMETER THAT CORRESPONDS TO THE BIAS OF THE LINEAR REGRESSION MODEL.
        self.BIAS = 0
        # # W_POST: IT'S THE POSTERIOR MEAN VECTOR.
        # self.W_POST = None
        # # S_POST: IT'S THE POSTERIOR COVARIANCE MATRIX.
        # self.S_POST = None

    # # FIT(): FITS THE BAYESIAN RIDGE REGRESSION MODEL TO THE TRAINING DATA
    # def FIT(self, X, Y):
    #     # D: NUMBER OF FEATURES: IT'S THE NUMBER OF FEATURES PLUS ONE BECAUSE OF THE BIAS TERM
    #     D = X.shape[1]
    #     # I: IDENTITY MATRIX: IT'S USED TO COMPUTE THE POSTERIOR COVARIANCE MATRIX
    #     I = np.identity(D)
    #     # S: COVARIANCE MATRIX: IT'S THE INVERSE OF THE PRECISION MATRIX
    #     S = np.linalg.inv(X.T @ X + self.ALPHA_2 * I)
    #     # W: MEAN VECTOR: IT'S THE PRODUCT OF THE PRECISION MATRIX AND THE PRODUCT OF THE FEATURES AND TARGET VALUES
    #     W = self.ALPHA_1 * S @ X.T @ Y
    #     # T: PRECISION MATRIX: IT'S THE SUM OF THE PRECISION MATRIX OF THE WEIGHTS AND THE PRECISION MATRIX OF THE NOISE
    #     T = self.LAMBDA_1 * I + self.LAMBDA_2 * (X.T @ X + self.ALPHA_2 * I)
    #     # W_POST: POSTERIOR MEAN VECTOR: IT'S THE PRODUCT OF THE PRECISION MATRIX AND THE PRODUCT OF THE FEATURES AND TARGET VALUES
    #     W_POST = np.linalg.inv(T) @ W
    #     # S_POST: POSTERIOR COVARIANCE MATRIX: IT'S THE INVERSE OF THE PRECISION MATRIX
    #     S_POST = np.linalg.inv(T)
    #     # SET THE INSTANCE VARIABLES W_POST AND S_POST AS THE POSTERIOR MEAN AND COVARIANCE
    #     self.W_POST = W_POST # POSTERIOR MEAN: IT'S THE PRODUCT OF THE PRECISION MATRIX AND THE PRODUCT OF THE FEATURES AND TARGET VALUES
    #     self.S_POST = S_POST # POSTERIOR COVARIANCE: IT'S THE INVERSE OF THE PRECISION MATRIX

    # # PREDICT(): PREDICTS THE TARGET VALUES FOR THE GIVEN TEST DATA
    # def PREDICT(self, X):
    #     # N: NUMBER OF SAMPLES
    #     N = X.shape[0]
    #     # Y_PRED: PREDICTED TARGET VALUES
    #     Y_PRED = np.zeros(N)
    #     # COMPUTE THE MEAN AND VARIANCE OF THE PREDICTIVE DISTRIBUTION FOR EACH TEST POINT
    #     for SAMPLE in range(N): # FOR EACH TEST POINT
    #         # COMPUTE THE MEAN OF THE PREDICTIVE DISTRIBUTION AS X @ W_POST
    #         Y_PRED[SAMPLE] = X[SAMPLE] @ self.W_POST # PREDICTED TARGET VALUE
    #     # RETURN THE MEAN AND VARIANCE OF THE PREDICTIVE DISTRIBUTION FOR EACH TEST POINT AS THE PREDICTED TARGET VALUES
    #     return Y_PRED

    # FIT(): FITS THE BAYESIAN RIDGE REGRESSION MODEL TO THE TRAINING DATA
    def FIT(self, X, Y):
        # INITIALIZES THE WEIGHTS AND THE BIAS
        # WEIGHTS: IT'S THE PARAMETER THAT CORRESPONDS TO THE WEIGHTS OF THE LINEAR REGRESSION MODEL.
        self.WEIGHTS = np.zeros(X.shape[1])
        # BIAS: IT'S THE PARAMETER THAT CORRESPONDS TO THE BIAS OF THE LINEAR REGRESSION MODEL.
        self.BIAS = 0
        # IMPLEMENTS THE GRADIENT DESCENT ALGORITHM
        for _ in range(self.N_ITER):
            # UPDATES THE WEIGHTS
            self.WEIGHTS = np.linalg.inv(
                self.ALPHA_2 * np.identity(X.shape[1]) + self.LAMBDA_2 * np.dot(X.T, X)).dot(
                self.LAMBDA_2 * np.dot(X.T, Y) + self.ALPHA_2 * self.WEIGHTS)
            # UPDATES THE BIAS
            self.BIAS = (Y - np.dot(X, self.WEIGHTS)).sum() / \
                (X.shape[0] * self.LAMBDA_2 + self.ALPHA_2)

    # PREDICT(): PREDICTS THE LABELS OF THE DATA
    def PREDICT(self, X):
        # RETURNS THE PREDICTED LABELS
        return np.dot(X, self.WEIGHTS) + self.BIAS