import numpy as np

# IMPLEMENTS THE A SINGLE NAIVE BAYES ALGORITHM.
class Bayes:
    # INITIALIZES THE BAYES ALGORITHM.
    def __init__(self, C = 1):
        self.C = C # REJECTION THRESHOLD: C. ABOVE THIS THRESHOLD, THE SAMPLE IS CLASSIFIED AS SPAM, OTHERWISE IT'S CLASSIFIED AS HAM.
        self.B, self.P = None, None # THRESHOLD DESIGNED TO OFFSET THE THRESHOLD TO A 0 VALUE (COMPARISSON VALUE), B, AND PROBABILITIES, P, IN R^{2 x N}.
    
    # TRAIN(): TRAINS THE MODEL; X REPRESENTS A VECTOR OF SAMPLES, Y REPRESENTS A VECTOR OF CORRESPONDING LABELS (1 FOR SPAM, 0 FOR HAM). EACH SAMPLE IS A DICIONARY WHERE THE KEY IS THE WORD AND THE VALUE IS THE ABSOLUTE FREQUENCY (COUNT) OF THE WORD IN THE SAMPLE.
    def TRAIN(self, X, Y):
        # COMPUTE M, M_HAM, M_SPAM, N:
        M_SPAM = np.sum(Y == 1) # NUMBER OF SPAM SAMPLES: M_SPAM
        M_HAM = np.sum(Y == -1) # NUMBER OF HAM SAMPLES: M_HAM
        M = M_SPAM + M_HAM # TOTAL NUMBER OF SAMPLES: M
        N = len(X[0]) # NUMBER OF WORDS IN THE VOCABULARY: N
        # INITIALIZE THE B = LOG C + LOG M_HAM - LOG M_SPAM TO OFFSET THE REJECTION THRESHOLD.
        self.B = np.log(self.C) + np.log(M_HAM) - np.log(M_SPAM)
        # INITIALIZE THE PROBABILITIES, P IN R^{2 x N} WITH P_I_J = 1, W_SPAM = N, W_HAM = N:
        self.P = np.ones((2, N)) # PROBABILITIES, P IN R^{2 x N}
        W_SPAM = N # NUMBER OF WORDS IN THE SPAM VOCABULARY: W_SPAM
        W_HAM = N # NUMBER OF WORDS IN THE HAM VOCABULARY: W_HAM
        for I in range(M): # FOR EACH SAMPLE: X[I]
            if Y[I] == 1: # IF THE SAMPLE IS SPAM
                for J in range(N): # FOR EACH WORD IN THE VOCABULARY
                    self.P[0, J] += X[I][J] # INCREMENT THE NUMBER OF TIMES THE WORD APPEARS IN THE SPAM VOCABULARY: P_SPAM_J; HERE X_I_J DENOTES THE NUMBER OF TIMES THE WORD J APPEARS IN THE SAMPLE X[I].
                    W_SPAM += X[I][J] # INCREMENT THE NUMBER OF WORDS IN THE SPAM VOCABULARY: W_SPAM
            else: # IF THE SAMPLE IS HAM
                for J in range(N): # FOR EACH WORD IN THE VOCABULARY
                    self.P[1, J] += X[I][J] # INCREMENT THE NUMBER OF TIMES THE WORD APPEARS IN THE HAM VOCABULARY: P_HAM_J; HERE X_I_J DENOTES THE NUMBER OF TIMES THE WORD J APPEARS IN THE SAMPLE X[I].
                    W_HAM += X[I][J] # INCREMENT THE NUMBER OF WORDS IN THE HAM VOCABULARY: W_HAM
        # COMPUTE THE PROBABILITIES, P IN R^{2 x N}:
        self.P[0, :] /= W_SPAM # P_SPAM_J = P_SPAM_J / W_SPAM
        self.P[1, :] /= W_HAM # P_HAM_J = P_HAM_J / W_HAM
        # # COMPUTE THE LOG PROBABILITIES, P IN R^{2 x N}:
        # for J in range(N):
        #     self.P[0, J] = self.P[0, J] / W_SPAM # P_SPAM_J = P_SPAM_J / W_SPAM
        #     self.P[1, J] = self.P[1, J] / W_HAM # P_HAM_J = P_HAM_J / W_HAM

    # CLASSIFY(): PREDICTS THE OUTPUT OF A SINGLE SAMPLE: X; SAMPLE IS A DICIONARY WHERE THE KEY IS THE WORD AND THE VALUE IS THE ABSOLUTE FREQUENCY (COUNT) OF THE WORD IN THE SAMPLE. RETURNS 1 IF SPAM, 0 IF HAM.
    def CLASSIFY(self, X):
        T = -self.B # INITIALIZE THE SCORE THRESHOLD, T = -B.
        for J in range(len(X)): # FOR EACH WORD IN THE VOCABULARY
            T += X[J] * np.log(self.P[0, J] / self.P[1, J]) # INCREMENT THE SCORE THRESHOLD, T, BY THE LOG RATIO OF THE PROBABILITIES OF THE WORD J IN THE SPAM AND HAM VOCABULARIES.
        return 1 if T > 0 else -1 # IF THE SCORE THRESHOLD, T, IS GREATER THAN 0, THEN THE SAMPLE IS SPAM, OTHERWISE IT'S HAM.