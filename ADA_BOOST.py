import numpy as np

# DECISION_STUMP: CLASS THAT IMPLEMENTS DECISION STUMP MODEL
class DECISION_STUMP:
    # INITIALIZES THE DECISION STUMP MODEL
    def __init__(self):
        self.POLARITY = 1  # POLARITY: IT'S THE POLARITY OF THE DECISION STUMP.
        # FEATURE_IDX: IT'S THE INDEX OF THE FEATURE THAT THE DECISION STUMP USES.
        self.FEATURE_IDX = None
        # THRESHOLD: IT'S THE THRESHOLD OF THE DECISION STUMP.
        self.THRESHOLD = None
        self.ALPHA = None  # ALPHA: IT'S THE WEIGHT OF THE DECISION STUMP.

    # FIT_TRANSFORM(): METHOD THAT TRAINS AND TRANSFORMS THE DECISION STUMP MODEL
    def FIT_TRANSFORM(self, X):
        N_SAMPLES = X.shape[0]  # N_SAMPLES: NUMBER OF SAMPLES
        # X_COLUMN: IT'S THE COLUMN OF THE FEATURE THAT THE DECISION STUMP USES.
        X_COLUMN = X[:, self.FEATURE_IDX]
        # HYPOTHESES: IT'S THE ARRAY THAT CONTAINS THE PREDICTED CLASS LABELS OF THE DATASET.
        HYPOTHESES = np.ones(N_SAMPLES)
        if self.POLARITY == 1:  # IF THE POLARITY IS 1
            # CHANGES THE CLASS LABELS OF THE SAMPLES THAT ARE LESS THAN THE THRESHOLD
            HYPOTHESES[X_COLUMN < self.THRESHOLD] = -1
        else:  # IF THE POLARITY IS -1
            # CHANGES THE CLASS LABELS OF THE SAMPLES THAT ARE GREATER THAN THE THRESHOLD
            HYPOTHESES[X_COLUMN > self.THRESHOLD] = -1
        return HYPOTHESES  # RETURNS THE TRANSFORMED DATASET

# ADA_BOOST: CLASS THAT IMPLEMENTS ADA_BOOST MODEL
class ADA_BOOST:
    # INITIALIZES THE ADA_BOOST MODEL
    def __init__(self, N_CLF=5):
        # N_CLF: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE NUMBER OF CLASSIFIERS.
        self.N_CLF = N_CLF
        # CLFS: IT'S THE ARRAY THAT CONTAINS THE CLASSIFIERS.
        self.CLFS = []

    # FIT(): METHOD THAT TRAINS THE ADA_BOOST MODEL
    def FIT(self, X, Y):
        # N_SAMPLES: NUMBER OF SAMPLES, N_FEATURES: NUMBER OF FEATURES
        N_SAMPLES, N_FEATURES = X.shape
        # W: IT'S THE ARRAY THAT CONTAINS THE WEIGHTS OF THE SAMPLES.
        WEIGHTS = np.full(N_SAMPLES, (1 / N_SAMPLES))
        self.CLFS = []  # CLFS: IT'S THE ARRAY THAT CONTAINS THE CLASSIFIERS.
        # ITERATE THROUGH CLASSIFIERS
        for _ in range(self.N_CLF):
            CLF = DECISION_STUMP()  # CLF: IT'S THE CLASSIFIER.
            MIN_ERROR = float("inf")  # MIN_ERROR: IT'S THE MINIMUM ERROR.
            # ITERATE THROUGH FEATURES
            for FEATURE_I in range(N_FEATURES):
                # X_COLUMN: IT'S THE COLUMN OF THE FEATURE.
                X_COLUMN = X[:, FEATURE_I]
                # THRESHOLDS: IT'S THE ARRAY THAT CONTAINS THE UNIQUE VALUES OF THE COLUMN.
                THRESHOLDS = np.unique(X_COLUMN)
                # ITERATE THROUGH THRESHOLDS
                for THRESHOLD in THRESHOLDS:
                    # PREDICT WITH POLARITY 1
                    P = 1
                    # PREDICTIONS: IT'S THE ARRAY THAT CONTAINS THE PREDICTED CLASS LABELS OF THE DATASET.
                    PREDICTIONS = np.ones(N_SAMPLES)
                    # CHANGES THE CLASS LABELS OF THE SAMPLES THAT ARE LESS THAN THE THRESHOLD
                    PREDICTIONS[X_COLUMN < THRESHOLD] = -1
                    # MISCLASSIFIED: IT'S THE ARRAY THAT CONTAINS THE MISCLASSIFIED SAMPLES.
                    MISCLASSIFIED = WEIGHTS[Y != PREDICTIONS]
                    # ERROR: IT'S THE ERROR OF THE CLASSIFIER.
                    ERROR = sum(MISCLASSIFIED)
                    if ERROR > 0.5:  # IF THE ERROR IS GREATER THAN 0.5
                        ERROR = 1 - ERROR  # ERROR = 1 - ERROR
                        P = -1  # P = -1
                    # STORE THE BEST CONFIGURATION
                    if ERROR < MIN_ERROR:
                        CLF.POLARITY = P  # SAVE POLARITY
                        CLF.THRESHOLD = THRESHOLD  # SAVE THRESHOLD
                        CLF.FEATURE_IDX = FEATURE_I  # SAVE FEATURE INDEX
                        MIN_ERROR = ERROR  # SAVE MINIMUM ERROR
            # CALCULATE ALPHA AND SAVE IT
            CLF.ALPHA = 0.5 * np.log((1.0 - MIN_ERROR) / (MIN_ERROR + 1e-10))
            # CALCULATE PREDICTIONS AND UPDATE WEIGHTS
            PREDICTIONS = CLF.FIT_TRANSFORM(X)
            # UPDATE WEIGHTS: W = W * e^(-ALPHA * Y * PREDICTIONS)
            WEIGHTS *= np.exp(-CLF.ALPHA * Y * PREDICTIONS)
            # NORMALIZE WEIGHTS (SUM OF WEIGHTS = 1)
            WEIGHTS /= np.sum(WEIGHTS)
            # SAVE CLASSIFIER
            self.CLFS.append(CLF)

    # TRANSFORM(): METHOD THAT TRANSFORMS THE DATA
    def TRANSFORM(self, X):
        # CLF_PREDS: IT'S THE ARRAY THAT CONTAINS THE PREDICTIONS OF THE CLASSIFIERS.
        CLF_PREDS = [CLF.ALPHA * CLF.FIT_TRANSFORM(X) for CLF in self.CLFS]
        # HYPOTHESIS: IT'S THE ARRAY THAT CONTAINS THE HYPOTHESIS OF THE MODEL.
        HYPOTHESIS = np.sum(CLF_PREDS, axis=0)
        return np.sign(HYPOTHESIS)  # RETURNS THE SIGN OF THE HYPOTHESIS