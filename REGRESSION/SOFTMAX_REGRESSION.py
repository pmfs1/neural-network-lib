import numpy as np
from NEURAL_NETWORKS.ACTIVATIONS import SOFTMAX

# SOFTMAX_REGRESSION: CLASS THAT IMPLEMENTS SOFTMAX REGRESSION MODEL
class SOFTMAX_REGRESSION:
    # INITIALIZES THE SOFTMAX REGRESSION MODEL
    def __init__(self, LEARNING_RATE=0.01, C=2, EPOCHS=1000):
        # LEARNING RATE: HYPERPARAMETER THAT CONTROLS THE STEP SIZE AT EACH ITERATION WHILE MOVING TOWARDS A MINIMUM OF A LOSS FUNCTION.
        self.LEARNING_RATE = LEARNING_RATE
        # NUMBER OF CLASSES: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE NUMBER OF CLASSES IN THE DATASET.
        self.C = C
        # NUMBER OF ITERATIONS: IT'S THE HYPERPARAMETER THAT CORRESPONDS TO THE NUMBER OF ITERATIONS THAT THE ALGORITHM PASS THROUGH THE TRAINING DATA.
        self.EPOCHS = EPOCHS
        # WEIGHTS: IT'S THE PARAMETER THAT CORRESPONDS TO THE WEIGHTS OF THE POLYNOMIAL REGRESSION MODEL.
        self.WEIGHTS = None
        # BIAS: IT'S THE PARAMETER THAT CORRESPONDS TO THE BIAS OF THE POLYNOMIAL REGRESSION MODEL.
        self.BIAS = None

    # ONE_HOT: CONVERTS THE LABELS TO ONE HOT ENCODING FORMAT
    def ONE_HOT(self, Y):
        # Y: LABELS/GROUND TRUTH
        # C: NUMBER OF CLASSES
        # INITIALIZING THE ONE HOT ENCODING MATRIX
        Y_HOT = np.zeros((len(Y), self.C))
        # CONVERTING THE LABELS TO ONE HOT ENCODING FORMAT
        Y_HOT[np.arange(len(Y)), Y] = 1
        return Y_HOT  # RETURNING THE ONE HOT ENCODING MATRIX

    # FIT(): TRAINS THE SOFTMAX REGRESSION MODEL
    def FIT(self, X, Y):
        # NUMBER OF TRAINING EXAMPLES AND NUMBER OF FEATURES
        NUMBER_OF_SAMPLES, NUMBER_OF_FEATURES = X.shape
        self.WEIGHTS = np.random.random(
            (NUMBER_OF_FEATURES, self.C))  # INITIALIZING THE WEIGHTS
        self.BIAS = np.random.random(self.C)  # INITIALIZING THE BIAS
        LOSSES = []  # LIST TO STORE THE LOSS AT EACH EPOCH
        for _ in range(self.EPOCHS):  # LOOPING THROUGH THE EPOCHS
            Z = X@self.WEIGHTS + self.BIAS  # CALCULATING THE HYPOTHESIS/PREDICTION
            # CALCULATING THE SOFTMAX OF THE HYPOTHESIS/PREDICTION
            HYPOTHESIS = SOFTMAX(Z)
            # CONVERTING THE LABELS TO ONE HOT ENCODING FORMAT
            Y_HOT = self.ONE_HOT(Y)
            # CALCULATING THE GRADIENT OF THE LOSS W.R.T WEIGHTS
            W_GRADIENT = (1 / NUMBER_OF_SAMPLES) * \
                np.dot(X.T, (HYPOTHESIS - Y_HOT))
            # CALCULATING THE GRADIENT OF THE LOSS W.R.T BIAS
            B_GRADIENT = (1 / NUMBER_OF_SAMPLES) * np.sum(HYPOTHESIS - Y_HOT)
            self.WEIGHTS -= self.LEARNING_RATE * W_GRADIENT  # UPDATING THE WEIGHTS
            self.BIAS -= self.LEARNING_RATE * B_GRADIENT  # UPDATING THE BIAS
            # CALCULATING THE LOSS
            LOSS = -np.mean(np.log(HYPOTHESIS[np.arange(len(Y)), Y]))
            LOSSES.append(LOSS)  # APPENDING THE LOSS TO THE LIST

    # TRANSFORM(): PREDICTS THE LABELS FOR THE GIVEN DATA
    def TRANSFORM(self, X):
        Z = X@self.WEIGHTS + self.BIAS  # CALCULATING THE HYPOTHESIS/PREDICTION
        # CALCULATING THE SOFTMAX OF THE HYPOTHESIS/PREDICTION
        HYPOTHESIS = SOFTMAX(Z)
        return np.argmax(HYPOTHESIS, axis=1)  # RETURNING THE PREDICTED LABELS