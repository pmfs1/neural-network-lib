import numpy as np

# MULTINOMIAL_NAIVE_BAYES: CLASS THAT IMPLEMENTS THE MULTINOMIAL NAIVE BAYES ALGORITHM
class MULTINOMIAL_NAIVE_BAYES:
    # INITIALIZES THE MULTINOMIAL NAIVE BAYES ALGORITHM
    def __init__(self, C=1):
        # REJECTION THRESHOLD: C. ABOVE THIS THRESHOLD, THE SAMPLE IS CLASSIFIED AS SPAM, OTHERWISE IT'S CLASSIFIED AS HAM.
        self.C = C
        # THRESHOLD DESIGNED TO OFFSET THE THRESHOLD TO A 0 VALUE (COMPARISSON VALUE), B, AND PROBABILITIES, P, IN R^{2 x N}.
        self.B, self.P = None, None

    # FIT(): TRAINS THE MODEL; X REPRESENTS A VECTOR OF SAMPLES, Y REPRESENTS A VECTOR OF CORRESPONDING LABELS (1 FOR SPAM, 0 FOR HAM). EACH SAMPLE IS A DICIONARY WHERE THE KEY IS THE WORD AND THE VALUE IS THE ABSOLUTE FREQUENCY (COUNT) OF THE WORD IN THE SAMPLE.
    def FIT(self, X, Y):
        # COMPUTE M, M_HAM, M_SPAM, N:
        M_SPAM = np.sum(Y == 1)  # NUMBER OF SPAM SAMPLES: M_SPAM
        M_HAM = np.sum(Y == -1)  # NUMBER OF HAM SAMPLES: M_HAM
        M = M_SPAM + M_HAM  # TOTAL NUMBER OF SAMPLES: M
        N = len(X[0])  # NUMBER OF WORDS IN THE VOCABULARY: N
        # INITIALIZE THE B = LOG C + LOG M_HAM - LOG M_SPAM TO OFFSET THE REJECTION THRESHOLD.
        self.B = np.log(self.C) + np.log(M_HAM) - np.log(M_SPAM)
        # INITIALIZE THE PROBABILITIES, P IN R^{2 x N} WITH P_I_J = 1, W_SPAM = N, W_HAM = N:
        self.P = np.ones((2, N))  # PROBABILITIES, P IN R^{2 x N}
        W_SPAM = N  # NUMBER OF WORDS IN THE SPAM VOCABULARY: W_SPAM
        W_HAM = N  # NUMBER OF WORDS IN THE HAM VOCABULARY: W_HAM
        for I in range(M):  # FOR EACH SAMPLE: X[I]
            if Y[I] == 1:  # IF THE SAMPLE IS SPAM
                for J in range(N):  # FOR EACH WORD IN THE VOCABULARY
                    # INCREMENT THE NUMBER OF TIMES THE WORD APPEARS IN THE SPAM VOCABULARY: P_SPAM_J; HERE X_I_J DENOTES THE NUMBER OF TIMES THE WORD J APPEARS IN THE SAMPLE X[I].
                    self.P[0, J] += X[I][J]
                    # INCREMENT THE NUMBER OF WORDS IN THE SPAM VOCABULARY: W_SPAM
                    W_SPAM += X[I][J]
            else:  # IF THE SAMPLE IS HAM
                for J in range(N):  # FOR EACH WORD IN THE VOCABULARY
                    # INCREMENT THE NUMBER OF TIMES THE WORD APPEARS IN THE HAM VOCABULARY: P_HAM_J; HERE X_I_J DENOTES THE NUMBER OF TIMES THE WORD J APPEARS IN THE SAMPLE X[I].
                    self.P[1, J] += X[I][J]
                    # INCREMENT THE NUMBER OF WORDS IN THE HAM VOCABULARY: W_HAM
                    W_HAM += X[I][J]
        # COMPUTE THE PROBABILITIES, P IN R^{2 x N}:
        self.P[0, :] /= W_SPAM  # P_SPAM_J = P_SPAM_J / W_SPAM
        self.P[1, :] /= W_HAM  # P_HAM_J = P_HAM_J / W_HAM

    # TRANSFORM(): PREDICTS THE OUTPUT OF A SINGLE SAMPLE: X; SAMPLE IS A DICIONARY WHERE THE KEY IS THE WORD AND THE VALUE IS THE ABSOLUTE FREQUENCY (COUNT) OF THE WORD IN THE SAMPLE. RETURNS 1 IF SPAM, 0 IF HAM.
    def TRANSFORM(self, X):
        assert self.B is not None and self.P is not None, 'ERROR: THE MODEL HAS NOT BEEN TRAINED.'
        T = -self.B  # INITIALIZE THE SCORE THRESHOLD, T = -B.
        for J in range(len(X)):  # FOR EACH WORD IN THE VOCABULARY
            # INCREMENT THE SCORE THRESHOLD, T, BY THE LOG RATIO OF THE PROBABILITIES OF THE WORD J IN THE SPAM AND HAM VOCABULARIES.
            T += X[J] * np.log(self.P[0, J] / self.P[1, J])
        # IF THE SCORE THRESHOLD, T, IS GREATER THAN 0, THEN THE SAMPLE IS SPAM, OTHERWISE IT'S HAM.
        return 1 if T > 0 else -1