import numpy as np

# QUADRATIC_DISCRIMINANT_ANALYSIS: IMPLEMENTATION OF QUADRATIC DISCRIMINANT ANALYSIS. QUADRATIC DISCRIMINANT ANALYSIS (QDA) IS A METHOD USED WHEN YOU HAVE A SET OF PREDICTOR VARIABLES AND YOUâ€™D LIKE TO CLASSIFY A RESPONSE VARIABLE INTO TWO OR MORE CLASSES. IT IS CONSIDERED TO BE THE NON-LINEAR EQUIVALENT TO LINEAR DISCRIMINANT ANALYSIS. QDA ASSUMES THAT EACH CLASS FOLLOWS A GAUSSIAN DISTRIBUTION. THE CLASS-SPECIFIC PRIOR IS SIMPLY THE PROPORTION OF DATA POINTS THAT BELONG TO THE CLASS. THE CLASS-SPECIFIC MEAN VECTOR IS THE AVERAGE OF THE INPUT VARIABLES THAT BELONG TO THE CLASS.


class QUADRATIC_DISCRIMINANT_ANALYSIS:
    # INITIALIZES THE QUADRATIC DISCRIMINANT ANALYSIS MODEL.
    def __init__(self):
        # CLASSES: IT'S THE LIST OF CLASSES.
        self.CLASSES = None
        # PRIORS: IT'S THE DICTIONARY THAT CONTAINS THE PRIORS OF THE CLASSES.
        self.PRIORS = None
        # MEANS: IT'S THE DICTIONARY THAT CONTAINS THE MEANS OF THE CLASSES.
        self.MEANS = None
        # COVS: IT'S THE DICTIONARY THAT CONTAINS THE COVARIANCES OF THE CLASSES.
        self.COVS = None

    # FIT(): IT'S THE FUNCTION THAT TRAINS THE QUADRATIC DISCRIMINANT ANALYSIS MODEL.
    def FIT(self, X, Y):
        # CLASSES: IT'S THE LIST OF CLASSES.
        self.CLASSES = np.unique(Y)
        # PRIORS: IT'S THE DICTIONARY THAT CONTAINS THE PRIORS OF THE CLASSES.
        self.PRIORS = {}
        # MEANS: IT'S THE DICTIONARY THAT CONTAINS THE MEANS OF THE CLASSES.
        self.MEANS = {}
        # COVS: IT'S THE DICTIONARY THAT CONTAINS THE COVARIANCES OF THE CLASSES.
        self.COVS = {}
        # LOOP OVER THE CLASSES.
        for CLASS in self.CLASSES:
            # X_C: IT'S THE MATRIX THAT CONTAINS THE DATA POINTS THAT BELONG TO THE CLASS.
            X_C = X[Y == CLASS]
            # PRIORS: IT'S THE DICTIONARY THAT CONTAINS THE PRIORS OF THE CLASSES.
            self.PRIORS[CLASS] = X_C.shape[0] / X.shape[0]
            # MEANS: IT'S THE DICTIONARY THAT CONTAINS THE MEANS OF THE CLASSES.
            self.MEANS[CLASS] = np.mean(X_C, axis=0)
            # COVS: IT'S THE DICTIONARY THAT CONTAINS THE COVARIANCES OF THE CLASSES.
            self.COVS[CLASS] = np.cov(X_C, rowvar=False)

    # PREDICT(): IT'S THE FUNCTION THAT PREDICTS THE CLASSES OF THE DATA POINTS.
    def PREDICT(self, X):
        # PREDS: IT'S THE LIST THAT WILL CONTAIN THE PREDICTED CLASSES.
        PREDS = []
        # LOOP OVER THE DATA POINTS.
        for _X in X:
            # POSTS: IT'S THE LIST THAT WILL CONTAIN THE POSTERIORS OF THE DATA POINTS.
            POSTS = []
            # LOOP OVER THE CLASSES.
            for CLASS in self.CLASSES:
                # PRIOR: IT'S THE LOGARITHM OF THE PRIOR OF THE CLASS.
                PRIOR = np.log(self.PRIORS[CLASS])
                # INV_COV: IT'S THE INVERSE OF THE COVARIANCE OF THE CLASS.
                INV_COV = np.linalg.inv(self.COVS[CLASS])
                # INV_COV_DET: IT'S THE DETERMINANT OF THE INVERSE OF THE COVARIANCE OF THE CLASS.
                INV_COV_DET = np.linalg.det(INV_COV)
                # DIFF: IT'S THE DIFFERENCE BETWEEN THE DATA POINT AND THE MEAN OF THE CLASS.
                DIFF = _X - self.MEANS[CLASS]
                # LIKELIHOOD: IT'S THE LOGARITHM OF THE LIKELIHOOD OF THE DATA POINT.
                LIKELIHOOD = 0.5 * np.log(INV_COV_DET) - \
                    0.5 * DIFF.T @ INV_COV @ DIFF
                # POST: IT'S THE LOGARITHM OF THE POSTERIOR OF THE DATA POINT.
                POST = PRIOR + LIKELIHOOD
                # APPEND THE POSTERIOR TO THE LIST OF POSTERIORS.
                POSTS.append(POST)
            # PRED: IT'S THE CLASS THAT MAXIMIZES THE POSTERIOR.
            PRED = self.CLASSES[np.argmax(POSTS)]
            # APPEND THE CLASS TO THE LIST OF CLASSES.
            PREDS.append(PRED)
        # RETURN THE LIST OF CLASSES.
        return np.array(PREDS)
